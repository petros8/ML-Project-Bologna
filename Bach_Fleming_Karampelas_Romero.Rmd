---
title: "Machine Learning"
author: 
  - Bach, Helena ^[Università di Bologna, 975492]
  - Fleming, María^[Università di Bologna, xxx]
  - Karampelas, Petro^[Università di Bologna, 943760]
  - Romero, Pablo José^[Università di Bologna, xxx]
date: "19/01/2022"
output:
  pdf_document:
    extra_dependencies: ["float"]
    number_sections: False
header-includes:
  \usepackage{floatrow}
  \usepackage{amsmath}
  \usepackage{apacite}
  \usepackage{natbib}
  \floatsetup[figure]{capposition=top}
  \usepackage[font={large}]{caption}
    
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "H", out.extra = "")
```

```{r packages, warning=FALSE, message=FALSE, echo=FALSE}

##options(java.parameters = "-Xmx21g")

library(corrplot)
library(caTools)
library(biotools)
library(ggplot2)
library(MASS)
library(xtable)
library(dplyr)
library(tidyr)
library(caret)
library(kableExtra)
library(BayesTree)
library(tree)
library(e1071)
library(pROC)



```

# Abstract
This project uses a dataset about a Portuguese Banking Marketing Campaign, collecting demographic and social-economic  information of the customer as well as records of direct marketing campaigngs phone calls. The goal of the project is to compare the performance of different models when predicting whether a customer will subscribe or not to a term deposit and identify which variables are more relevant. We consider a Lasso Logistic, Neural Networks, Classification Trees, Random Forests and Support Vector Mechanism.  Some of the hyperparameters of the models are determined through cross-validation.  To assess the execution of each model we use the ROC curve and the AUC. Random forest is the model that performs better with an AUC of 0.928. 
|   Random Forests and Decision Trees allow us to perform feature selection and to have an idea of what variables are more important when determining the success of the marketing campaign. It has been concluded that the two main variables are \emph{duration} and \emph{poutcomesuccess}.

# Introduction 
Can machines be trained to predict the success of marketing campaigns? In this paper, we attempt to address this question using the Portuguese Bank Marketing dataset available in the UCI Machine Learning Repository. The dataset relates to direct phone call marketing campaigns, which aimed to promote term deposits among existing customers, by a Portuguese banking institution from May 2008 to November 2010. The dataset contains data specific to each customer as well as information related with the response of the marketing campaign. 
We will use these variables to train  models to be able to predict and classify which clients are  more likely to subscribe to the term deposit marketed in the campaign based on their response in these phone calls and their characteristics. 
For this purpose, Helena used a regularized logistic model on the dataset as a reference point, and then she supplemented  this linear model with the more sophisticated non linear algorithm Neural Networks. Petros and Maria focused on Binomial Trees and Random Forest, while Pablo concentrated on Support Vector Machines.  

# Descriptive Analysis
|   The dimentionality of the dataset is such that it contains 45.211 observations across 17  variables, 11 of which are categorical and the remaining 6 are numerical. As the variables in the dataset are scaled differently, to enhance comparability, we re-scale the data and encode categorical variables to be treated as dummy variables. Moreover we duplicate the original dataset so we normalize the variables between 0 and 1 and use this dataset in further analysis 

```{r categorical, results='hide'}

bank <- read.csv(file="bank-full.csv", header=TRUE, sep=";")
#variables that should be treated as categorical
bank$marital <- as.factor(bank$marital)
bank$education <- as.factor(bank$education)
bank$default <- as.factor(bank$default)
bank$housing <- as.factor(bank$housing)
bank$job <- as.factor(bank$job)
bank$month <- as.factor(bank$month)
bank$contact <- as.factor(bank$contact)
bank$loan <- as.factor(bank$loan)
bank$poutcome <- as.factor(bank$poutcome)
bank$y <- as.factor(bank$y)
#scale numeric variables
bank %>%
mutate_if(is.numeric, scale)
#normalize
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
bank_n <- mutate_if(bank, is.numeric, normalize)
#Dummy encoding
dummy_vars <- dummyVars(" ~ .", data=bank, fullRank=TRUE)
bank_m <- data.frame(predict(dummy_vars, newdata=bank))
bank_nn<-data.frame(predict(dummy_vars, newdata=bank_n))

```

We define the train and test sample in order to build and evaluate the model using different data. 

```{r test, results='hide'}

#create test and training sample
set.seed(101)
sample<- sample(1:nrow(bank), nrow(bank)*0.75)
train <- bank_m[sample, ]
test <- bank_m[-sample, ]

```
Figure \ref{fig:boxplot} shows the box plot of the numerical variables. It can be noticed that \emph{age} follows a very similar distribution for clients who subscribed to a term deposit and for those who did not, indicating that the age of the customer won't play a significant role when predicting the behavior of clients. On the other hand, from the box-plot of \emph{duration} it can be inferred that high duration (long contact duration in second) might have a positive impact on the subscription rate. 

```{r boxplot, boxplot, dev='png', fig.cap="\\label{fig:boxplot}Boxplot", out.width="60%", echo=FALSE}

par(mfrow=c(3,2))
boxplot(age ~ y, data = bank, main="Age")
boxplot(balance ~ y, data = bank, main="Balance")
boxplot(day ~ y, data = bank, main="Day")
boxplot(duration ~ y, data = bank, main="Duration")
boxplot(pdays ~ y, data = bank, main="Pdays")
boxplot(previous ~ y, data = bank, main="Previous")

```

It is interesting to also have a graphical idea of how our variable of interest (whether the client subscribes or not) is distributed.

```{r propy, dev='png', fig.cap="\\label{fig:prop}Proportion", out.width="60%", eco=FALSE}

barplot(prop.table(table(bank_m$y.yes))) 

```

The plot shown above in Figure \ref{fig:prop} evidences that we have imbalanced data. The average of our dependent dummy variable is 0.12, meaning that we only have 12% of observations that belong to class 1 (yes).

# Logit
|   The logistic regression models the probability of belonging to one class, under the assumption that the dependent variable follows a binomial distribution. It is a classification technique that allows to predict a dichotomous variable. 
Given $X$ (explanatory variables) we can represent the probability  that the client has subscribed to a term deposit (Y) as $p(X)=P(y=yes|X)$, using the logistic function to ensure that the output lies between 0 and 1.  
\begin{center}
$p(X)=\frac{e^{\beta_0+\beta_1 \cdot X}}{1+e^{\beta_0 + \beta_1\cdot X}}$
\end{center}
Logistic models are fit through maximum likelihood: $l(\beta)=\sum_{i=1}^{N}{[y_i \beta^T x_i - \log (1+e^{\beta^T x_i})]}$
As it has been mentioned before, the dataset used in this project contains 17 variables. It is desirable to work with a reduced set of variables. For this purpose we will perform regularization through the LASSO approach. The logistic LASSO regression regularizes by  maximizing  a penalized version of the previous written equation for the likelihood function: 
\begin{center}
$l(\beta)=\sum_{i=1}^{N}{[y_i (\beta_0 \beta ^T x_i - \log (1+e^{\beta_0 + \beta^T x_i}) - \lambda \sum_{j=1}^p \lvert \beta_k \lvert}]$ 
\end{center}
The coefficients contributing less to the model are forced to 0, allowing for only the most significant variables to be analyzed.
```{r logitmod, message=FALSE, warning=FALSE, echo=FALSE}

x.train <- as.matrix(train[,-43])
y.train <- cbind(train$y.yes)
x.test <- as.matrix(test[,-43])
y.test <- cbind(test$y.yes)
library(glmnet)

```

The tuning parameter $\lambda$ is data dependent and therefore crucial to choose its value correctly. To assist us in making the correct choice, we perform cross-validation. As the value of $\lambda$ increases, the regularization effect is strengthened . The function \emph{cv.glmnet} carries out 10-fold cross validation  by default, such that it divides  our sample into 10 folds to determine the value of $\lambda$ yielding the most predictive model. 

```{r lambdaplot}

#find the optimal lambda using cross-validation 
set.seed(1)
cv.lambda <- cv.glmnet(x.train,y.train, alpha=1, family = "binomial", 
                       type.measure="mse", expand.grid=c(10^-4, 1)) 

```
```{r lambaplot, dev='png', fig.cap="\\label{fig:lambda} Lambda", out.width="60%"}

plot(cv.lambda)

```

The plot in Figure \ref{fig:lambda} demonstrates how the Mean-Squared Error changes for different values of lambda. As $\lambda$ increases, variables are excluded from the model and the MSE increases. \emph{lambda.min} is the $\lambda$ that minimizes the Mean Cross-Validation Error. However, it is recommended to choose the $\lambda$ that is withing one standard error of the minimum, so to choose the simplest model (most regularized model) whose accuracy is equivalent to the best model.

```{r logit, message=FALSE}

#we choose the regularization factor lambda which minimizes the mean squared error
print(cv.lambda$lambda.min)
print(cv.lambda$lambda.1se)
#we fit the logistic model with a binomial dependent variable
mylogit <-  glmnet(x.train,y.train,family = "binomial", alpha =1, lambda=cv.lambda$lambda.1se)
#we can get the coefficients of the regularized model
coef(mylogit, cv.lambda$lambda.1se)

```

The output above shows how the LASSO shrinks some coefficients to 0. Interestingly, \emph{age} shrinks 0, in line with the comment done in the descriptive analysis, suggesting that the age of the customer has no significant effect when determining whether he/she will subscribe to the term deposit advertised. 
|   To evaluate the performance of the LASSO logistic model defined previously, we  could create a confusion matrix and compare the predicted values obtained to the actual values. However, it is important to account for the fact that we have an imbalanced  data. Because of the unequal distribution of classes, the accuracy measure is not reliable. For this reason, we use the ROC curve. The ROC curve summarizes the performance of a binary classification by plotting the False Positive Rate against the True Positive Rate. The Area Under the curve (AUC) is used as a diagnostic tool to compare different classification models. 

```{r logitpred}

#prediction
probabilities <- mylogit %>% predict(x.test)
test$predicted <- as.vector(ifelse(probabilities > 0.5, 1, 0))

```
```{r rocplot, dev='png', fig.cap="\\label{fig:auc} ROC plot", out.width="60%", message=FALSE}
# #AUC
g <- roc(test$y.yes, test$predicted, plot=TRUE, print.auc=TRUE)
confusionMatrix(table(test$y.yes, test$predicted))

```
Figure \ref{fig:auc}  shows the ROC plot for the LASSO logistic regression with the corresponding AUC. An AUC of 0.5 indicates that the model is not able to discriminate between "yes" and "no".  Our AUC is equal to 0.60 which means that the model can correctly distinguish between the two classes with a 60\% chance. If we compute the confusion matrix, we obtain an accuracy of 0.897 and high measures of Sensitivity but very low performance of Negative Predicted Values (0.216). This exemplifies the use of AUC metric to account for imbalanced data. 

# Neural Networks
Neural Networks are biological inspired algorithms that recognize non-linear relationships between variables. In this project we will focus on Supervised learning (as we have labeled data) and we will use  a Multi-Layer Neural Network. The algorithm works from left to right, starting in the input layer. In the output layer we use the Sigmoid activation function to keep values between 0 and 1, as we want to predict 2 different classes indicated as 0 and 1. 
  Before proceeding to  the model  we ensured we normalised the numeric inputs to a standard scale so to improve the NN'accuracy.  Next, We redefined the train and test sample on the scaled and normalized dataset. 
  
```{r scale, message=FALSE, warning=FALSE, echo=FALSE}
library(tensorflow)
library(keras)
bank_nn$y.yes <- to_categorical(bank_nn$y.yes, num_classes=2)
set.seed(101)
sample = sample.split(bank[,1], SplitRatio = .75)
train <- bank_nn[sample, ]
test <- bank_nn[-sample, ]
x.train <- as.matrix(train[,-43])
y.train <- cbind(train$y.yes)
x.test <- as.matrix(test[,-43])
y.test <- cbind(test$y.yes)
```

The capacity of the neural network to learn is rooted in its design, that is, the number of layers and nodes. We use the \emph{keras} package to model our NN.  For simplicity, we have decided to model a Neural Network with one only hidden layer and we have used cross-validation to choose the number of neurons and the dropout rate. Adding a dropout rate  avoids overfitting by randomly selecting neurons and set them equal to 0 with a given probability. The dropout rate that is commonly used is $50\%$. However, there is some evidence that low rates perform better. For this reason, we decided to try two different dropout rates: $10\%$ and $50\%$.

```{r runs, message=FALSE, warning=FALSE, results='hide'}
library(tfruns)
runs <- tuning_run("Runs.R", 
                  flags = list(dense_units2=c(16, 32),
                                dropout1=c(0.1, 0.5)))
```

In the first part of the code we define our model: we specify the layers (input, hidden and output) and how they are going to behave (by specifying the activation function). Notice that in the input and hidden layer we use the Rectified Linear (ReLu) function, which is the default activation function.  However, in the output layer we specify the Sigmoid activation function as our output variables ranges from 0 to 1. In the second part of the code, we compile our model, defining the loss function and the metrics. The \emph{binary cross Entropy} loss function is used in binary classification problems and we specify AUC as the metric. In the last part, the model is trained through the \emph{fit} function. The process is iterated 10 different times and, to validate we are not overfitting, we hold out a 10% of the dataset. 

```{r cv, dev='png', fig.cap="\\label{fig:auc} ROC plot", out.width="80%"}

for (i in 1:4){
model <- keras_model_sequential() %>%
  # Input layer
  layer_dense(units = runs$flag_dense_units2[i], activation = "relu", input_shape = 42) %>%
  # Hidden layer
  layer_dense(units = runs$flag_dense_units2[i], activation = "relu") %>%
  #Dropout layer (to avoid overfitting)
  layer_dropout(rate = runs$flag_dropout1[i])  %>%
  # Output layer
  layer_dense(units = 2, activation = "sigmoid")
model %>% compile( 
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = tf$keras$metrics$AUC())
#we inspect the training history 
history <- fit(
  object           = model,
  x                = x.train, 
  y                = y.train,
  batch_size       = 35, 
  epochs           = 10,
  validation_split = 0.10
)
#we predict
model_pred<- as.integer(predict(model, x.test) > 0.5)
assign(paste("model_pred",i), model_pred)
}

```
We repeat the process 4 times in a loop in order to get the ROC curve for each model considered and be able to choose the optimal hyperparameter. 
```{r plotcvv, dev='png', fig.cap="\\label{fig:plotcv} ROC plot", out.width="60%", message=FALSE, warning=FALSE, echo=FALSE}

par(mfrow=c(2,2))
g <- roc(test$y.yes, `model_pred 1`, plot=TRUE, print.auc=TRUE, main="32 nodes, 0.5 rate")
g1 <-roc(test$y.yes, `model_pred 2`, plot=TRUE, print.auc=TRUE, main="16 nodes, 0.5 rate")
g2 <-roc(test$y.yes, `model_pred 3`, plot=TRUE, print.auc=TRUE, main="32 nodes, 0.1 rate")
g3 <-roc(test$y.yes, `model_pred 4`, plot=TRUE, print.auc=TRUE, main="16 nodes, 0.1 rate")
```

Figure \ref{fig:plotcv} displays the ROC plot with its respective AUC for each of the 4 models. It can be seen that the one that performs better is the model containing 32 nodes with a dropout rate of 0.1 
Figure \ref{fig:nn} shows the structure of our  Neural Network. We have 42 inputs (42 independent variables) and one hidden layer with 32 nodes. 

```{r neurallib, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
library(nnet)
library(devtools)
nn<-nnet(y.yes~., data=train, size=32, decay=5e-4, maxit=100, MaxNWts = 3000)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')

```

```{r neuralplot,dev='png', fig.cap="\\label{fig:nn} Neural Network", out.width="60%", message=FALSE, warning=FALSE }

plot.nnet(nn)

```
It can be seen that the Neural Network we designed performs better than the logistic regression as its AUC is higher, indicating that it is able to better distinguish between the two classes. However, Neural Networks have a main drawback: they are computationally expensive to train. In this project we have simplified the problem by designing a model with one single hidden layer and defining two possible number of neurons and dropout rates. Another drawback of Neural Networks that they can be difficult to interpret. In the logistic regression, the coefficient's size carried information regarding the importance of the variable in the classification result. 
  Neural Networks seem to have a higher discrimination power and classification performance but the model cannot be interpreted. 
  
# Trees
Decision trees are a supervised learning method that can either be used for regression or classification problems. In this project we make use of the classification version of tree methods, as we want to predict a qualitative response variable.Their biggest advantage is the ease of interpreting the results, which can also be visualized and explained to a person who is not an expert in machine learning. In their simplest form, decision trees don't perform as well compared to other models such as linear regression. However, there are ways to vastly improve their predictability.
For this project, 3 tree variants are applied and compared:
|   1. A simple tree
|   2. Pruning the above tree, using the results of cross validation with the misclassification error as a criterion
|   3. Random Forest

Before assessing the results, the two latter methods are ensemble methods, meaning that many decision trees are being created and each tree's
result is then aggregated to make a prediction for the output.

```{r intro, message=FALSE, warning=FALSE, echo=FALSE }
#One simple tree
set.seed(101)
sample <- sample.split(bank[,1], SplitRatio = 0.75)
train <- bank[sample, ]
test <- bank[-sample, ]
```

```{r tree, warning=FALSE,dev='png', fig.cap="\\label{fig:onetree} One simple tree", out.width="60%", message=FALSE, warning=FALSE }

library(tree)
onetree <- tree(train$y ~.,train[,c(1:16)],split='deviance')
summary(onetree)
plot(onetree)
text(onetree , pretty = 0)
predict_onetree <- predict(onetree,test, type = "class")

```
The first tree shown in Figure \ref{fig:onetree} is obtained through an algorithm that performs recursive partition of the predictor space with minimizing the deviance as a criterion in each step. This is a greedy, top-down approach as it performs this minimization in each partition. According to this model , the variable that could be used as good determinant of the outcome is duration. Also, the outcome of the previous marketing campaign seems to be important, as we infer that if a customer already has a certificate it is less likely for him to purchase another one.  
Trees like this usually overfit the data and as a result they have high variance. This can also be corroborated by the depth of the tree, because trees that are this deep somewhat overfit. This is why cost-complexity pruning is performed on this tree, with the hopes of seeing a difference in performance and possibly obtaining a different tree structure and enriching our understanding of the data.

```{r cvtrees, message=FALSE, warning=FALSE, echo=FALSE }
#Cross-Validation and Pruned tree
crosstree <- cv.tree(onetree, FUN = prune.misclass)
crosstree
prunetree <-prune.misclass(onetree, best = 4)
```
In Figure \ref{fig:prune}, we perform cost-complexity pruning using the number of misclassifications as the criterion, as proposed in the recommended textbook.
After obtaining the results, we prune the tree so that it has 4 terminal nodes. We find out that duration is considered the most important variable, followed by \emph{poutcome}
```{r plotprunetree, ,dev='png', fig.cap="\\label{fig:prune} Prune Tree", out.width="60%", message=FALSE, warning=FALSE }
plot(prunetree)
text(prunetree)
predict_prunetree <- predict(prunetree,test, type = "class")
```

```{r plotauc, ,dev='png', fig.cap="\\label{fig:auctrees} ROC curve", out.width="60%", message=FALSE, warning=FALSE }
par(mfrow=c(1,2))
s1 <- roc(test$y, as.numeric(predict_prunetree), plot=TRUE, print.auc=TRUE, main="Prune Tree")
s2 <-roc(test$y, as.numeric(predict_onetree), plot=TRUE, print.auc=TRUE, main= "Simple Tree")
```
From Figure \ref{fig:auctrees} it can be seen that both trees have a close metric, however the first tree scores better, as it captures more information from the data. Hypothetically, if the pruning parameters (terminal nodes) changed then this result could be improved. Nonetheless, a reason for doing pruning apart from performance is interpretability, as it helps us map the most important variables. 

# Random Forest
Random forest is an ensemble machine learning algorithm that operates to build multiple decision trees that work both for regression and classification. Random forests are similar to bagged trees such that the random forest algorithm builds a number of decision trees on bootstrapped training samples. Random forests improve upon bagged trees such that the they have an added improvement that they de-correlates the trees which allow for greater accuracy. Moreover, when building these decision trees, each time a split in a tree occurs, a new random sample of mtry predictors is picked, these randomly chosen mtry predictors then become potential candidates for splitting from the full set of p predictors.
```{r rforest, echo=FALSE}
inTrain <- createDataPartition(bank$y, p = 0.75, list = FALSE)
bank_train <- bank[inTrain, ]
bank_test <- bank[-inTrain, ]
```
We make use of the \emph{tuneRF} function to find the optimal number of variables randomly sampled as candidates at each split (mtry). The default value for classification is $\sqrt{p}$. In our case, the hyperparameter mtry can take any value from 1 to 17 (the number of predictors) so we expect the best value to be near $$\sqrt{17}\sim 4 $$ Figure \ref{fig:mtry} shows that the optimal value of predictors to be used is 4 (minimize the Out-Of-Bag error). 
```{r cvforest with rf package, fig.cap="\\label{fig:mtry} Cross Validation Random Forest", out.width="60%", message=FALSE}
library(randomForest)
#Cross-validation for optimal mtry value(size of slitting variable subset)
cvforest <- tuneRF(bank_train[,c(1:16)], bank_train[,17],mtryStart=4,ntreeTry=100, stepFactor=2, improve=0.01,
trace=TRUE, plot=TRUE, doBest=FALSE)
```
Having defined the optimal number of variables to be considered, we can proceed by creating a randomForest object:
```{r randomforest, dev='png', fig.cap="\\label{fig:trees} Number of Trees", out.width="60%", message=FALSE}
forest <- randomForest(bank_train[,c(1:16)], bank_train[,17], ntree=150,mtry=4,importance=TRUE)
pred <- predict(forest,bank_test[,c(1:16)], type="vote",
norm.votes=FALSE, predict.all=FALSE, nodes=TRUE)
```
The \emph{randomForest} function will create an object that contains votes. These votes are based on the Out of Bag (OOB) sample tree classification votes for each data point. These votes roughly represent a probability, and therefore can be used to create a ROC and AUC measure.
```{r treeroc, dev='png', fig.cap="\\label{fig:aucrf} ROC curve", out.width="60%", message=FALSE}
require(pROC)
rock<-roc(bank_train$y,forest$votes[,2],print.auc=TRUE, plot=TRUE)
```
From Figure \ref{fig:aucrf} we can see the ROC curve and the AUC metric obtained for the Random Forest. It evidences that the model is highly able to correctly classify (92.8\% of chance) and distinguish observations of the two different classes.  
|   One important advantage of Random Forest and Trees is that, differently from Neural Networks, these models offer a simple way to visualize the relative importance of the variables. Estimating the feature importance can provide us with a better understanding of the problem and simplify the model.
```{r cvforest2, dev='png', fig.cap="\\label{fig:RF2} Most Important Variables: Random Forest", out.width="60%", message=FALSE}
ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)
mtryGrid <- data.frame(mtry = floor(seq(10, ncol(bank_train), length = 10)))
set.seed(22)
rf3 <- caret::train(y ~., data = bank_train, method = "rf", tuneGrid = mtryGrid, ntree = 200, importance = TRUE, trControl = ctrl,verbose=TRUE)
rf_imp <- varImp(rf3, scale = FALSE, competes = FALSE)
rf_imp
plot(rf_imp)
```
From Figure \ref{fig:RF2} it can be seen that the most important variable is \emph{duration} (last contact duration), followed by \emph{poutcomesuccess}. Thus, longer calls can be an indicator of the customer's interest in the product and be a sign of success of the marketing campaign. Also, taking into account the outcome of previous campaigns can also give us an insight of the result of the current marketing campaign. 
Next, we can use the Random forest to predict between the two classes.
```{r cvforest3, dev='png', fig.cap="\\label{fig:RF3} Random Forest: Predictions", out.width="60%"}
rf_pred <- predict(rf3, newdata = bank_test)
confusionMatrix(rf_pred, bank_test$y)
plot(rf_pred)
```
The data above displays some performance measures of the classification model. Before we've seen that the AUC is the highest we have obtained so far. We can also observe that the other indicators are also very high. 

# Support Vector Machine
The main idea of a support vector machine (SVM) is that it uses an hyperplane to classify observation in two classes.   It uses non-linear kernels to enlarge a feature space to address non-linearity in the original space. It is an extension of the support vector classifier that uses a hyperplane in the presence of a linear boundary for classifying an observation in two classes. In doing so, it takes into account a cost parameter that represents the "size" of the violations of the margin that is formed between the hyperplane and the nearest observations of the support vector classifiers. This can be written as:

$$f(x) = \beta_0+\sum_{i=1}^n\alpha_i\langle x, x_i \rangle$$
where $\langle x, x_i \rangle$ represents the inner product between the observation. Hence, \textit{a priori} there are $\binom{n}{2}$ inner products to be computed. Nevertheless, the $\alpha_i$ is nonzero only for the support vectors, so we could rewrite $f(x)$ as follows:

$$ f(x) = \beta_0+\sum_{i\epsilon S}\alpha_i\langle x, x_i \rangle $$
where $S$ is the subset of indices of the support vectors.
This $f(x)$ determines the predicted class label for a test observation based on the sign of the function.
   The kernel function, of the form $K(x_i,x_{i'})$, quantifies the similarity between two observations. Common examples of kernels in SVM are linear, polynomial, radial and sigmoidal kernels. Combining support vector classifier with non-linear kernels results in the support vector machine classifier.
   
|  First, we will fit a Support Vector Classifier using a linear kernel in order to compare the radial and polynomial kernels that
give us the Support Vector Machine. Then we will compare the misclasification erros and AUC-ROC curves to decide which of these 
kernels is more appropriate.

```{r svm.fit, warning = FALSE, message= FALSE}

svmfit.linear <- svm(y ~ ., data = train, kernel = "linear", cost = 1, decision.values = TRUE)
svmfit.radial <- svm(y ~ ., data = train, kernel = "radial",
                    gamma = 1, cost = 1, decision.values = TRUE)
svmfit.poly <- svm(y ~ ., data = train, kernel = "radial",
                    degree = 3, cost = 1, decision.values = TRUE)
```

The numbers of support vectors in the linear, radial and polynomial kernel are `r svmfit.linear$tot.nSV` , `r svmfit.radial$tot.nSV` and
`r svmfit.poly$tot.nSV`, respectively,

```{r merror.svm}
pred.linear <- predict(svmfit.linear, newdata = test)
pred.radial <- predict(svmfit.radial, newdata = test)
pred.poly <- predict(svmfit.poly, newdata = test)
merror.linear <- table(pred.linear,test$y)
merror.radial <- table(pred.radial,test$y)
merror.poly <- table(pred.poly,test$y)
merrors.svm <- c(sum(merror.linear[1,2],merror.linear[2,1])/ sum(merror.linear),
                    sum(merror.radial[1,2],merror.radial[2,1])/ sum(merror.radial),
                    sum(merror.poly[1,2],merror.poly[2,1])/ sum(merror.poly))

names(merrors.svm) <- c("Linear (SVC)","Radial (SVM)","Polynomial(SVM)")

minerror <- min(merrors.svm)

```


|  Observing only the misclassification error we would choose the polynomial kernel ($d=3$) with a misclassification error of `r round(minerror,4)`.


```{r aucroc.svm,message = FALSE, dev='png', fig.cap="\\label{fig:auc.svm} ROC plot SVM", out.width="60%", echo=FALSE}

par(mfrow = c(3,1))

rocauc.linear <- roc(test$y, as.numeric(pred.linear),
                     plot=TRUE, print.auc=TRUE)


rocauc.radial <- roc(test$y, as.numeric(pred.radial),
                     plot=TRUE, print.auc=TRUE)


rocauc.poly <- roc(test$y, as.numeric(pred.poly),
                   plot=TRUE, print.auc=TRUE)



```

The first plot of \ref{fig:auc.svm} is the ROC curve generated from the linear kernel, 
the second one is from the radial kernel and the third is from the polynomial. The AUC are, in order, `r rocauc.linear$auc[1]`
, `r rocauc.radial$auc[1]` and `r rocauc.poly$auc[1]`. Following the AUC-ROC curve criterion we should choose
the support vector classifier and not the polynomial support vector machine.
|   One of them main disadvantages of the SVM method is that, as the support vector classifier works using the relative distance of observations, there is no probabilistic explanation to our models. However, this method is highly effective in high dimensional spaces, such as the one generated from our data set.

# Conclusion:
In this paper we have used machine learning  to predict and classify clients between two classes, that is we presented and attempted to train 5 models, logistical Regression , Neural Networks, Decision Trees, Random Forest and Structural Vector Machines to be able to predict which  clients are more likely to respond well to the marketing campaign posed by the bank and subscribe to a term deposit. 
\begin{tabular}{|r|r|r|r|r|r|}
\hline
 & Logistic Regression & Neural Networks & Trees & Random Forest & SVM\\
\hline
AUC & 0.602 & 0.895 & 0.697 & 0.928 & 0.609 \\
\hline
\end{tabular}
The table above summarizes the AUC obtained in each model. As it has been previously mentioned, the AUC measures the ability of the model to distinguish between the two different classes. High values of AUC indicate that the classifier is capable of correctly discriminate the categories.  In the table above, it can be seen that Random Forest is the model that performs better, followed by Neural Networks. 
Random Forest also gave us an insight about the variables that have a more significant role when determining the effect of the marketing campaign. The \emph{duration} and the success of previous campaigns (\emph{poutcomesuccess}) have a strong effect when determining the behavior of the customer.  This information can be useful when planning future campaigns to reduce costs and have a better idea of what customers to target. 
Further analysis, in further studies, we would try to ensemble logistical regression, Neural Networks and Decision Trees as due to time constraints , it was outside the scope of this paper. 


\newpage

\bibliographystyle{apacite}
\renewcommand{\bibname}{Referencias} 
\nocite{*}
\bibliography{references.bib}



